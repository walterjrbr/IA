# Projetos Pessoais em Inteligência Artificial  
### Desenvolvidos de forma independente desde Maio de 2025 
[LinkedIn](https://www.linkedin.com/in/walter-augusto-jr-76425623/) 

## Sobre
Este repositório reúne estudos e experimentos realizados com inteligência artificial e métodos computacionais aplicados à física, biologia, incerteza estatística e aprendizado por reforço com LLMs. Os notebooks documentam explorações conceituais e técnicas.


## Notebooks incluídos

### `2LTermodinâmica.ipynb`
Estudo de segunda lei da termodinâmica com simulações computacionais. Explora conceitos fundamentais da física estatística com suporte computacional.

- Assunto: Termodinâmica, entropia, sistemas isolados
- Ferramentas: Python, Numpy, simulações numéricas

---

### `AZR.ipynb`
Implementação exploratória do sistema **Absolute Zero Reasoning** (AZR), com autojogos, indução via LLMs e geração de programas por reforço.

- Assunto: Raciocínio simbólico com LLMs, aprendizado por reforço com autojogo
- Base: Artigo *"Absolute Zero: Reinforced Self-Play Reasoning with Zero Data"*

---

### `UQ.ipynb`
Introdução à **Quantificação de Incertezas (Uncertainty Quantification)** aplicada à modelagem matemática e simulações.

- Assunto: Propagação de incerteza, Monte Carlo, Sensibilidade
- Ferramentas: Python, UQpy, SciPy

---

### `VAE.ipynb`
Treinamento de um **Variational Autoencoder (VAE)** com LSTM para geração de moléculas a partir de SMILES reais, voltado para aplicações em neurociência e bioinformática.

- Assunto: Geração molecular, deep learning, SMILES
- Ferramentas: PyTorch, RDKit, VAE, LSTM

---
### `RAG.ipynb`
Protótipo de **Retrieval Augmented Generation (RAG)** para responder perguntas com base em documentos próprios, usando embeddings e modelos de linguagem.

- Assunto: RAG, LangChain, vetorização semântica, contexto externo  
- Ferramentas: LangChain, FAISS, DeepSeek

---
`Fine_tuning_LORA_V2.ipynb`

Fine-tuning de LLMs com QLoRA + PEFT para tarefas de classificação e geração.

Este notebook apresenta um pipeline de fine-tuning leve de modelos de linguagem (LLMs) utilizando QLoRA (Quantized Low-Rank Adaptation) em conjunto com a biblioteca PEFT.
A proposta é adaptar LLMs para tarefas supervisionadas de classificação e geração de texto, com baixo custo computacional.

Técnicas: QLoRA (quantização em 4 bits + LoRA), PEFT (adaptação eficiente de parâmetros)

Tarefas: Classificação de texto e geração condicional

Ferramentas: Hugging Face Transformers, peft, bitsandbytes, LLM

## Em desenvolvimento

- Protótipo de agente autônomo com memória e raciocínio usando LangGraph

---

## Contato
Caso tenha interesse em colaborar, contratar ou conversar sobre IA aplicada:
- Disponível para oportunidades em IA, ciência de dados ou pesquisa básica/aplicada
