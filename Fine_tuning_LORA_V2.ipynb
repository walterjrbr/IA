{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # evita erro em padding\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config={\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "        \"bnb_4bit_use_double_quant\": True,\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    },\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade fsspec datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a054ad772cf4eeba8c5762ac09fe517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3e323c69a347d2b07c7947391c62de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b2aa16ea7b49bbb160a135e9f574b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3741361c384d1e8a6edf43d5c53d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb9ba17e47e4a8a8248986c09442e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a0fa37416f425c9086a0380e77ab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eaa6294aaf4a2e84d4fe3081e19c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"imdb\", split=\"train[:1%]\")\n",
    "print(raw_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552046bf0e334b2292521a586e0892ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdee4c1f5894fb5869b8afb921bec5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164a8e95e0cc41e595a634de5ce9ec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.769300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely loved this movie. It was fantastic!\n",
      "Sentiment: 10/10\n",
      "Overall:\n"
     ]
    }
   ],
   "source": [
    "# 1. Instalar pacotes (roda no Colab)\n",
    "# !pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "\n",
    "# 2. Importar libs\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# 3. Carregar dataset\n",
    "raw_dataset = load_dataset(\"imdb\", split=\"train[:80%]\")\n",
    "\n",
    "positives = raw_dataset.filter(lambda x: x[\"label\"] == 1).select(range(500))\n",
    "negatives = raw_dataset.filter(lambda x: x[\"label\"] == 0).select(range(500))\n",
    "from datasets import concatenate_datasets\n",
    "raw_dataset = concatenate_datasets([positives, negatives]).shuffle(seed=42)\n",
    "\n",
    "\n",
    "# 4. Carregar tokenizer e modelo TinyLlama quantizado em 4bit\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config={\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_compute_dtype\": torch.float16,\n",
    "        \"bnb_4bit_use_double_quant\": True,\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    },\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 5. Preprocessar dataset para treino causal LM\n",
    "def preprocess_function(example):\n",
    "    text = example[\"text\"]\n",
    "    label = example[\"label\"]\n",
    "    label_str = \"positive\" if label == 1 else \"negative\"\n",
    "    prompt = f\"Review: {text}\\nSentiment:\"\n",
    "    completion = f\" {label_str}\"\n",
    "\n",
    "    # Tokeniza prompt e completion juntos (entrada + saída)\n",
    "    full_text = prompt + completion\n",
    "    encodings = tokenizer(full_text, truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "    # labels = input_ids para causal LM\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "    return encodings\n",
    "\n",
    "train_dataset = raw_dataset.map(preprocess_function, remove_columns=raw_dataset.column_names)\n",
    "\n",
    "# 6. Configurar LoRA para fine tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # ajuste caso use outro modelo\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 7. Argumentos do treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-imdb\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    max_steps = 20,\n",
    ")\n",
    "\n",
    "# 8. Data collator para causal LM (não mask language modeling)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 9. Criar Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 10. Treinar\n",
    "trainer.train()\n",
    "\n",
    "# 11. Testar geração simples\n",
    "model.eval()\n",
    "prompt = \"Review: I absolutely loved this movie. It was fantastic!\\nSentiment:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I cannot stay indifferent to Lars van Trier's films. I consider 'Breaking the Waves' nothing less than a masterpiece. I loved 'Dancer in the Night'. I admired the idea in 'Dogville' but the overall exercise looked to me too dry and too theatrical, less cinema. 'Europa' which I see only now was a famous film at its time, succeeded in the US the relative success of an European film and got the Oscar for the best foreign language movie, but did not survive well the time in my opinion. It is also a too much explicit and extrovert exercise in cinema art to my taste.<br /><br />The story has a level of ambiguity that cannot escape the viewer. Treating the period that immediately followed the second world war not in the black and white colors of victors and vanquished, of executioners and victims but as rather ambiguous times when people of both sides were fighting for survival in the aftermath of a catastrophic event that change the lives of nations and individuals forever is still a source of disputes even today, more such was novel and courageous two decades ago. Yet it is the means of expression that really do not appear fit to the task.<br /><br />The film seems to include a lot of quotes descending directly from the films of Hitchcock, especially his early films set in the pre-war Europe, with brave British spies fighting evil German spies on trains crossing at high speed the continent at dark. The trains were a symbol of the world and its conflicts with all their intensity and dramatism. Here the train also becomes the symbol of the first sparkles of the re-birth of Germany after war, of its might, of its obsession with order and regulation, of punctuality and civility. The characters that populate the train are far from being the classical spy stories good or bad guys. The principal character a young American of German origin coming to post-war Europe willing to be part of a process of help and reconciliation finds himself in an ambiguous world of destruction and corruption, with liberators looking more like oppressive occupiers, with the vanquished not resigned to their fate but rather willing to continue on the path of self-destruction, with love doubtfully mixed with treason.<br /><br />It is yet this classical film treatment that betrays the director in this case. The actions of the characters, especially of Leopold Kessler played by Jean-Marc Barr seem confused, and lack credibility. The overall cinematography seems to be not Hitchcock-like but rather from a bad imitation of Hitchcock in the late 30s. The usage of color over the black-and-white film used in the majority of the time in moments of emotional intensity is also too demonstrative. It is not that Van Trier does not master his artistic means, but he is too demonstrative, he seems to try too hard to show what a great filmmaker he is. He really is great, as he will show in some of his later films, but it will be left to the viewers to decide this alone.\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "example = raw_dataset[29]  # primeira linha\n",
    "print(\"Text:\", example[\"text\"])\n",
    "print(\"Label:\", example[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, example in enumerate(raw_dataset):\n",
    "  #   print(f\"Example {i}:\")\n",
    "  #  print(\"Text:\", example[\"text\"])\n",
    "  #  print(\"Label:\", example[\"label\"])\n",
    "  #  print(\"---------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do dataset: 1000\n",
      "Labels únicos: {0, 1}\n",
      "Quantidade por label:\n",
      "{0: 500, 1: 500}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamanho do dataset: {len(raw_dataset)}\")\n",
    "\n",
    "labels = [ex[\"label\"] for ex in raw_dataset]\n",
    "print(f\"Labels únicos: {set(labels)}\")\n",
    "print(f\"Quantidade por label:\")\n",
    "print({label: labels.count(label) for label in set(labels)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classify the sentiment of the following movie review as either \"positive\" or \"negative\".\n",
    "\n",
    "Review: I really do like so much this movie\n",
    "Sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely loved this movie. It was fantastic!\n",
      "Sentiment: 10/10\n",
      "Overall:\n"
     ]
    }
   ],
   "source": [
    "# 11. Testar geração simples\n",
    "model.eval()\n",
    "prompt = \"Review: I absolutely loved this movie. It was fantastic!\\nSentiment:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = \"Review: I absolutely loved this movie. It was fantastic!\\nSentiment:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=3,                  # limita a resposta\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False                    # geração determinística\n",
    "    )\n",
    "\n",
    "text_out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Pega só o que vem depois de \"Sentiment:\"\n",
    "sentiment = text_out.split(\"Sentiment:\")[-1].strip().split()[0]\n",
    "\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely loved this movie. It was fantastic!\n",
      "Predicted sentiment: unknown\n",
      "----------------------------------------\n",
      "Review: I really did not enjoy this film at all.\n",
      "Predicted sentiment: negative\n",
      "----------------------------------------\n",
      "Review: It was okay, not great but not terrible.\n",
      "Predicted sentiment: negative\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Seu modelo e tokenizer já devem estar carregados acima:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"seu-modelo\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"seu-modelo\")\n",
    "# model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def classify_sentiment(review_text):\n",
    "    prompt = (\n",
    "        f\"Review: {review_text}\\n\"\n",
    "        \"Sentiment (0=negative, 1=positive):\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,  # suficiente para \" 0\" ou \" 1\"\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    text_out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extrai o número após \"Sentiment:\"\n",
    "    try:\n",
    "        prediction = text_out.split(\"Sentiment (0=negative, 1=positive):\")[-1].strip().split()[0]\n",
    "        if prediction == \"1\":\n",
    "            return \"positive\"\n",
    "        elif prediction == \"0\":\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Testes com 3 frases\n",
    "reviews = [\n",
    "    \"I absolutely loved this movie. It was fantastic!\",\n",
    "    \"I really did not enjoy this film at all.\",\n",
    "    \"It was okay, not great but not terrible.\",\n",
    "]\n",
    "\n",
    "for rev in reviews:\n",
    "    print(f\"Review: {rev}\")\n",
    "    print(\"Predicted sentiment:\", classify_sentiment(rev))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(classify_sentiment(\"This movie was terrible and boring.\"))\n",
    "print(classify_sentiment(\"Absolutely fantastic and enjoyable film!\"))\n",
    "print(classify_sentiment(\"I don't know what to think about this movie.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"imdb\", split=\"train[:20%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: I absolutely loved this movie. It was fantastic!\n",
      "DEBUG >> Geração completa:\n",
      " Review: I absolutely loved this movie. It was fantastic!\n",
      "Sentiment (0=negative, 1=positive): 5/\n",
      "Predicted sentiment: unknown\n",
      "----------------------------------------\n",
      "\n",
      "Review: I really did not enjoy this film at all.\n",
      "DEBUG >> Geração completa:\n",
      " Review: I really did not enjoy this film at all.\n",
      "Sentiment (0=negative, 1=positive): 0\n",
      "\n",
      "Predicted sentiment: negative\n",
      "----------------------------------------\n",
      "\n",
      "Review: It was okay, not great but not terrible.\n",
      "DEBUG >> Geração completa:\n",
      " Review: It was okay, not great but not terrible.\n",
      "Sentiment (0=negative, 1=positive): 0\n",
      "\n",
      "Predicted sentiment: negative\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Certifique-se de já ter o modelo e tokenizer carregados:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"seu-modelo\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"seu-modelo\")\n",
    "# model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def classify_sentiment(review_text):\n",
    "    # Prompt compatível com o treinamento\n",
    "    prompt = (\n",
    "        f\"Review: {review_text}\\n\"\n",
    "        \"Sentiment (0=negative, 1=positive):\"\n",
    "    )\n",
    "\n",
    "    # Tokeniza e envia para o mesmo dispositivo do modelo\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decodifica saída do modelo\n",
    "    text_out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Exibe saída bruta para debug\n",
    "    print(\"DEBUG >> Geração completa:\\n\", text_out)\n",
    "\n",
    "    try:\n",
    "        # Extrai a predição depois de \"Sentiment:\"\n",
    "        prediction = text_out.split(\"Sentiment (0=negative, 1=positive):\")[-1].strip().split()[0]\n",
    "\n",
    "        # Mapeia predição para label\n",
    "        if prediction in [\"1\", \"10\", \"1/1\", \"10/10\", \"positive\"]:\n",
    "            return \"positive\"\n",
    "        elif prediction in [\"0\", \"0/1\", \"0/10\", \"negative\"]:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Testes com várias frases\n",
    "reviews = [\n",
    "    \"I absolutely loved this movie. It was fantastic!\",\n",
    "    \"I really did not enjoy this film at all.\",\n",
    "    \"It was okay, not great but not terrible.\",\n",
    "]\n",
    "\n",
    "for rev in reviews:\n",
    "    print(f\"\\nReview: {rev}\")\n",
    "    print(\"Predicted sentiment:\", classify_sentiment(rev))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: I liked this movie. It was very good!\n",
      "DEBUG >> Geração completa:\n",
      " Review: I liked this movie. It was very good!\n",
      "Sentiment (0=negative, 1=positive): 1.\n",
      "Predicted sentiment: positive\n",
      "----------------------------------------\n",
      "\n",
      "Review: I really did not enjoy this film at all.\n",
      "DEBUG >> Geração completa:\n",
      " Review: I really did not enjoy this film at all.\n",
      "Sentiment (0=negative, 1=positive): 0\n",
      "\n",
      "Predicted sentiment: negative\n",
      "----------------------------------------\n",
      "\n",
      "Review: It was okay, not great but not terrible.\n",
      "DEBUG >> Geração completa:\n",
      " Review: It was okay, not great but not terrible.\n",
      "Sentiment (0=negative, 1=positive): 0\n",
      "\n",
      "Predicted sentiment: negative\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Certifique-se de que o modelo e tokenizer estão carregados\n",
    "# Exemplo:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"seu-modelo\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"seu-modelo\")\n",
    "# model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def classify_sentiment(review_text):\n",
    "    # Prompt compatível com o modelo treinado\n",
    "    prompt = (\n",
    "        f\"Review: {review_text}\\n\"\n",
    "        \"Sentiment (0=negative, 1=positive):\"\n",
    "    )\n",
    "\n",
    "    # Prepara entrada\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Gera saída\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decodifica a saída do modelo\n",
    "    text_out = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"DEBUG >> Geração completa:\\n\", text_out)\n",
    "\n",
    "    try:\n",
    "        # Extrai o valor gerado após \"Sentiment:\"\n",
    "        prediction = text_out.split(\"Sentiment (0=negative, 1=positive):\")[-1].strip().split()[0]\n",
    "\n",
    "        # Interpreta a saída do modelo\n",
    "        if any(prediction.startswith(p) for p in [\"1\", \"5\", \"10\"]):\n",
    "            return \"positive\"\n",
    "        elif prediction.startswith(\"0\"):\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Lista de reviews para teste\n",
    "reviews = [\n",
    "    \"I liked this movie. It was very good!\",\n",
    "    \"I really did not enjoy this film at all.\",\n",
    "    \"It was okay, not great but not terrible.\",\n",
    "]\n",
    "\n",
    "# Aplica classificação em cada review\n",
    "for rev in reviews:\n",
    "    print(f\"\\nReview: {rev}\")\n",
    "    print(\"Predicted sentiment:\", classify_sentiment(rev))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
